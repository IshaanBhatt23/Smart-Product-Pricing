{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f131c56c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n",
      "File paths are set.\n",
      "Loading data...\n",
      "Data loaded in 11.14 seconds.\n",
      "Starting text feature engineering\n",
      "Cleaning text for TF-IDF...\n",
      "Engineering metadata features (length, words, caps)...\n",
      "Engineering IPQ feature...\n",
      "Engineering keyword features...\n",
      "Generating core TF-IDF features...\n",
      "Combining all engineered features...\n",
      "All features have been successfully combined.\n",
      "Pre-processing complete in 61.14 seconds.\n",
      "Final training features shape: (75000, 30007)\n",
      "Final testing features shape: (75000, 30007)\n",
      "\n",
      "Output variables are now ready for model training: x_train, y_train, x_test\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import string \n",
    "print(\"Libraries imported successfully.\")\n",
    "TRAIN_PATH = r'C:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\Dataset\\train.csv'\n",
    "TEST_PATH = r'C:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\Dataset\\test.csv'\n",
    "\n",
    "print(\"File paths are set.\")\n",
    "\n",
    "print(\"Loading data...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    train_df = pd.read_csv(TRAIN_PATH)\n",
    "    test_df = pd.read_csv(TEST_PATH)\n",
    "    \n",
    "    all_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "    all_df['original_content'] = all_df['catalog_content'].fillna('')\n",
    "    \n",
    "    print(f\"Data loaded in {time.time() - start_time:.2f} seconds.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"ERROR: Could not find the file. Please check the path. Details: {e}\")\n",
    "    exit()\n",
    "\n",
    "print(\"Starting text feature engineering\")\n",
    "start_time = time.time()\n",
    "print(\"Cleaning text for TF-IDF...\")\n",
    "all_df['clean_content'] = all_df['original_content'].str.lower()\n",
    "\n",
    "print(\"Engineering metadata features (length, words, caps)...\")\n",
    "all_df['text_length'] = all_df['original_content'].str.len()\n",
    "all_df['word_count'] = all_df['original_content'].apply(lambda x: len(x.split()))\n",
    "all_df['capital_ratio'] = all_df['original_content'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1e-6))\n",
    "\n",
    "print(\"Engineering IPQ feature...\")\n",
    "def extract_ipq(text):\n",
    "    text = str(text).lower()\n",
    "    patterns = [r'pack of (\\d+)', r'(\\d+)\\s*pack', r'(\\d+)\\s*count', r'set of (\\d+)', r'(\\d+)\\s*ct', r'(\\d+)\\s*pk']\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    return 1\n",
    "all_df['ipq'] = all_df['clean_content'].apply(extract_ipq)\n",
    "\n",
    "print(\"Engineering keyword features...\")\n",
    "keywords = {\n",
    "    'quality': ['premium', 'organic', 'heavy-duty', 'professional', 'gourmet', 'handmade', 'luxury'],\n",
    "    'bundling': ['set', 'bundle', 'kit', 'combo', 'pack'],\n",
    "    'condition': ['refurbished', 'new', 'generic', 'compatible']\n",
    "}\n",
    "\n",
    "for category, words in keywords.items():\n",
    "    all_df[f'kw_{category}'] = all_df['clean_content'].apply(lambda x: 1 if any(word in x for word in words) else 0)\n",
    "\n",
    "print(\"Generating core TF-IDF features...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=30000,\n",
    "    stop_words='english',\n",
    "    token_pattern=r'\\b[a-zA-Z0-9]+\\b'\n",
    ")\n",
    "text_features_tfidf = tfidf_vectorizer.fit_transform(all_df['clean_content'])\n",
    "\n",
    "print(\"Combining all engineered features...\")\n",
    "additional_features_df = all_df[[\n",
    "    'text_length', 'word_count', 'capital_ratio', 'ipq', \n",
    "    'kw_quality', 'kw_bundling', 'kw_condition'\n",
    "]]\n",
    "\n",
    "additional_features_sparse = csr_matrix(additional_features_df.values)\n",
    "x_full = hstack([text_features_tfidf, additional_features_sparse], format='csr')\n",
    "\n",
    "print(\"All features have been successfully combined.\")\n",
    "x_train = x_full[:len(train_df)]\n",
    "x_test = x_full[len(train_df):]\n",
    "y_train = np.log1p(train_df['price'])\n",
    "\n",
    "print(f\"Pre-processing complete in {time.time() - start_time:.2f} seconds.\")\n",
    "print(f\"Final training features shape: {x_train.shape}\")\n",
    "print(f\"Final testing features shape: {x_test.shape}\")\n",
    "print(\"\\nOutput variables are now ready for model training: x_train, y_train, x_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559d1940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "\n",
      " Creating validation set and defining parameters...\n",
      "\n",
      " Training the LightGBM model...\n",
      "Training until validation scores don't improve for 50 rounds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mlgb_params)\n\u001b[0;32m     24\u001b[0m early_stopping_callback \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mearly_stopping(stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx_train_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrmse\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping_callback\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training complete in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Saving model and processed data to disk...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\lightgbm\\sklearn.py:1398\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m   1382\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1383\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1395\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1396\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1397\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1398\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1400\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1401\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1402\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1403\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1404\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1405\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1406\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1407\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1411\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1413\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[1;32mc:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\lightgbm\\engine.py:322\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[0;32m    311\u001b[0m     cb(\n\u001b[0;32m    312\u001b[0m         callback\u001b[38;5;241m.\u001b[39mCallbackEnv(\n\u001b[0;32m    313\u001b[0m             model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m         )\n\u001b[0;32m    320\u001b[0m     )\n\u001b[1;32m--> 322\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    325\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\lightgbm\\basic.py:4155\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, train_set, fobj)\u001b[0m\n\u001b[0;32m   4152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[0;32m   4153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4154\u001b[0m _safe_call(\n\u001b[1;32m-> 4155\u001b[0m     \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4156\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4159\u001b[0m )\n\u001b[0;32m   4160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[0;32m   4161\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib \n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "print(\"\\n Creating validation set and defining parameters...\")\n",
    "x_train_part, x_val, y_train_part, y_val = train_test_split(\n",
    "    x_train, y_train, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 15000,\n",
    "    'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1, 'num_leaves': 40,\n",
    "    'verbose': -1, 'n_jobs': -1, 'seed': 42, 'boosting_type': 'gbdt',\n",
    "}\n",
    "\n",
    "print(\"\\n Training the LightGBM model...\")\n",
    "start_time = time.time()\n",
    "model = lgb.LGBMRegressor(**lgb_params)\n",
    "early_stopping_callback = lgb.early_stopping(stopping_rounds=50, verbose=True)\n",
    "\n",
    "model.fit(\n",
    "    x_train_part, y_train_part, eval_set=[(x_val, y_val)],\n",
    "    eval_metric='rmse', callbacks=[early_stopping_callback]\n",
    ")\n",
    "print(f\"Model training complete in {time.time() - start_time:.2f} seconds.\")\n",
    "print(\"\\n Saving model and processed data to disk...\")\n",
    "joblib.dump(model, 'lightgbm_model.pkl')\n",
    "print(\"Model saved to 'lightgbm_model.pkl'\")\n",
    "joblib.dump(x_test, 'x_test_processed.pkl')\n",
    "joblib.dump(test_df, 'test_df.pkl')\n",
    "print(\"Processed test data saved to 'x_test_processed.pkl' and 'test_df.pkl'\")\n",
    "print(\"\\n Training and saving process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3beb3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n",
      "\n",
      " Loading pre-trained model and processed data...\n",
      "Files loaded successfully in 2.61 seconds.\n",
      "\n",
      " Generating predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions generated in 63.10 seconds.\n",
      "\n",
      " Creating the submission file...\n",
      "\n",
      " All done! Submission file saved as 'submission.csv'\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"Libraries imported.\")\n",
    "print(\"\\n Loading pre-trained model and processed data...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    model = joblib.load('lightgbm_model.pkl')\n",
    "    x_test = joblib.load('x_test_processed.pkl')\n",
    "    test_df = joblib.load('test_df.pkl')\n",
    "    print(f\"Files loaded successfully in {time.time() - start_time:.2f} seconds.\")\n",
    "except FileNotFoundError:\n",
    "    print(\" ERROR: .pkl files not found. Please run the training script first.\")\n",
    "    exit()\n",
    "print(\"\\n Generating predictions...\")\n",
    "start_time = time.time()\n",
    "predictions_log = model.predict(x_test)\n",
    "predictions = np.expm1(predictions_log)\n",
    "predictions[predictions < 0] = 0\n",
    "print(f\"Predictions generated in {time.time() - start_time:.2f} seconds.\")\n",
    "print(\"\\n Creating the submission file...\")\n",
    "submission_df = pd.DataFrame({\n",
    "    'sample_id': test_df['sample_id'],\n",
    "    'price': predictions\n",
    "})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\n All done! Submission file saved as 'submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
