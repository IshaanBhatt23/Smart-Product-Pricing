{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb82e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup is complete.\n",
      "\n",
      " ENGINEERING ALL FEATURES\n",
      "Feature engineering complete in 97.68 seconds.\n",
      "Final training data shape: (75000, 31287)\n",
      "\n",
      "HYPERPARAMETER TUNING WITH OPTUNA\n",
      "\n",
      " FINDING BEST HYPERPARAMETERS WITH OPTUNA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-12 18:58:19,905] A new study created in memory with name: no-name-2a66e17f-9319-4c89-9faa-ccf18127daa1\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 19:23:14,469] Trial 0 finished with value: 0.6993860796916658 and parameters: {'learning_rate': 0.023720756935600704, 'num_leaves': 28, 'feature_fraction': 0.9855033467898788, 'bagging_fraction': 0.6035736005491347, 'bagging_freq': 5, 'lambda_l1': 1.5013765515260067e-08, 'lambda_l2': 1.2651180244396026e-07}. Best is trial 0 with value: 0.6993860796916658.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 20:02:33,012] Trial 1 finished with value: 0.688836736781493 and parameters: {'learning_rate': 0.029085173610015926, 'num_leaves': 88, 'feature_fraction': 0.7632082610542106, 'bagging_fraction': 0.6077864479063269, 'bagging_freq': 1, 'lambda_l1': 0.0006559987906164, 'lambda_l2': 1.7284339762592815e-06}. Best is trial 1 with value: 0.688836736781493.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 20:52:11,552] Trial 2 finished with value: 0.6874484507723471 and parameters: {'learning_rate': 0.09975535529115606, 'num_leaves': 96, 'feature_fraction': 0.9523270710962306, 'bagging_fraction': 0.7957226327767634, 'bagging_freq': 5, 'lambda_l1': 0.11130840695024667, 'lambda_l2': 8.834991695540857e-05}. Best is trial 2 with value: 0.6874484507723471.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 21:30:12,970] Trial 3 finished with value: 0.6857200993808813 and parameters: {'learning_rate': 0.08732454829179291, 'num_leaves': 87, 'feature_fraction': 0.7149856290188331, 'bagging_fraction': 0.9775230647981097, 'bagging_freq': 1, 'lambda_l1': 5.983318447109444e-08, 'lambda_l2': 3.053593563127868e-07}. Best is trial 3 with value: 0.6857200993808813.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 22:01:42,193] Trial 4 finished with value: 0.6901877688987067 and parameters: {'learning_rate': 0.07508160061608175, 'num_leaves': 70, 'feature_fraction': 0.8589318954303709, 'bagging_fraction': 0.8855519488526533, 'bagging_freq': 6, 'lambda_l1': 0.03579113739907782, 'lambda_l2': 8.993507308070901}. Best is trial 3 with value: 0.6857200993808813.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 22:31:34,776] Trial 5 finished with value: 0.6889987947641107 and parameters: {'learning_rate': 0.0314841565765779, 'num_leaves': 87, 'feature_fraction': 0.605900095997202, 'bagging_fraction': 0.6690728596855406, 'bagging_freq': 5, 'lambda_l1': 2.5453551824555167e-07, 'lambda_l2': 0.13657992166193855}. Best is trial 3 with value: 0.6857200993808813.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 23:12:23,238] Trial 6 finished with value: 0.6960891514555789 and parameters: {'learning_rate': 0.060999627915226125, 'num_leaves': 96, 'feature_fraction': 0.6394595038194585, 'bagging_fraction': 0.7298137438439888, 'bagging_freq': 2, 'lambda_l1': 5.107494512230503, 'lambda_l2': 1.222024894536417}. Best is trial 3 with value: 0.6857200993808813.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-12 23:38:33,238] Trial 7 finished with value: 0.6982609697333251 and parameters: {'learning_rate': 0.021991159652708862, 'num_leaves': 37, 'feature_fraction': 0.6206717677630679, 'bagging_fraction': 0.9739638577981007, 'bagging_freq': 3, 'lambda_l1': 7.865188320922472e-07, 'lambda_l2': 0.07324236137755932}. Best is trial 3 with value: 0.6857200993808813.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 00:13:58,675] Trial 8 finished with value: 0.6892396616775639 and parameters: {'learning_rate': 0.041545024717797835, 'num_leaves': 62, 'feature_fraction': 0.776096761844368, 'bagging_fraction': 0.9865367179609359, 'bagging_freq': 2, 'lambda_l1': 0.0005283330553696383, 'lambda_l2': 6.03101061062368e-07}. Best is trial 3 with value: 0.6857200993808813.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 00:40:03,273] Trial 9 finished with value: 0.6956802155192008 and parameters: {'learning_rate': 0.035384725433493136, 'num_leaves': 32, 'feature_fraction': 0.9543212666592829, 'bagging_fraction': 0.9266598730420122, 'bagging_freq': 6, 'lambda_l1': 0.005034132626523042, 'lambda_l2': 0.4445810029045405}. Best is trial 3 with value: 0.6857200993808813.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 01:16:19,740] Trial 10 finished with value: 0.6859810514178386 and parameters: {'learning_rate': 0.09438087911090587, 'num_leaves': 75, 'feature_fraction': 0.7020649967579203, 'bagging_fraction': 0.8568218101552956, 'bagging_freq': 1, 'lambda_l1': 1.062042548105988e-05, 'lambda_l2': 1.6918481470938574e-08}. Best is trial 3 with value: 0.6857200993808813.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 01:50:48,550] Trial 11 finished with value: 0.6853764966220776 and parameters: {'learning_rate': 0.09722720774670324, 'num_leaves': 76, 'feature_fraction': 0.7019611654078022, 'bagging_fraction': 0.8535582059038943, 'bagging_freq': 1, 'lambda_l1': 1.5215332047246604e-05, 'lambda_l2': 1.0383484990196821e-08}. Best is trial 11 with value: 0.6853764966220776.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 02:16:30,225] Trial 12 finished with value: 0.6880840411981645 and parameters: {'learning_rate': 0.08141081124319445, 'num_leaves': 49, 'feature_fraction': 0.6996536058514301, 'bagging_fraction': 0.8433857256447115, 'bagging_freq': 3, 'lambda_l1': 9.077667349783954e-06, 'lambda_l2': 1.2375015415525264e-05}. Best is trial 11 with value: 0.6853764966220776.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 02:52:36,918] Trial 13 finished with value: 0.6845614699296525 and parameters: {'learning_rate': 0.07906888885012742, 'num_leaves': 80, 'feature_fraction': 0.7076087319081708, 'bagging_fraction': 0.7896333991548031, 'bagging_freq': 1, 'lambda_l1': 1.4653540459992095e-08, 'lambda_l2': 1.076692023873937e-08}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 03:31:28,632] Trial 14 finished with value: 0.685736366362323 and parameters: {'learning_rate': 0.0682716800124764, 'num_leaves': 76, 'feature_fraction': 0.8467976417360819, 'bagging_fraction': 0.7766843348379636, 'bagging_freq': 3, 'lambda_l1': 1.1290059357157143e-05, 'lambda_l2': 1.5230603361801135e-08}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 03:59:16,078] Trial 15 finished with value: 0.6884093275411651 and parameters: {'learning_rate': 0.04972317308398936, 'num_leaves': 53, 'feature_fraction': 0.6707894006759422, 'bagging_fraction': 0.7434635166204825, 'bagging_freq': 2, 'lambda_l1': 1.3579770574423137e-06, 'lambda_l2': 0.00024988239877261344}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 04:31:24,046] Trial 16 finished with value: 0.6853014791174535 and parameters: {'learning_rate': 0.08727536661837858, 'num_leaves': 66, 'feature_fraction': 0.7450813259941554, 'bagging_fraction': 0.8345142740246251, 'bagging_freq': 4, 'lambda_l1': 1.2087313815058408e-08, 'lambda_l2': 0.007080373749006531}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 05:04:11,330] Trial 17 finished with value: 0.6891980744634665 and parameters: {'learning_rate': 0.07195428195642706, 'num_leaves': 62, 'feature_fraction': 0.8589394517512722, 'bagging_fraction': 0.7139848452659268, 'bagging_freq': 7, 'lambda_l1': 1.0101246580840574e-08, 'lambda_l2': 0.004555176812521497}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 05:30:41,731] Trial 18 finished with value: 0.686010309226413 and parameters: {'learning_rate': 0.0845956224808701, 'num_leaves': 47, 'feature_fraction': 0.8104739500025568, 'bagging_fraction': 0.9095716282043346, 'bagging_freq': 4, 'lambda_l1': 9.326102089182949e-08, 'lambda_l2': 0.00348736617674542}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 06:10:09,581] Trial 19 finished with value: 0.696887603782199 and parameters: {'learning_rate': 0.01161879108469472, 'num_leaves': 68, 'feature_fraction': 0.7379298572802507, 'bagging_fraction': 0.8132373436674691, 'bagging_freq': 4, 'lambda_l1': 1.959202927022947e-06, 'lambda_l2': 0.00580496391953191}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 06:49:35,273] Trial 20 finished with value: 0.6870127782646506 and parameters: {'learning_rate': 0.060686315560557424, 'num_leaves': 82, 'feature_fraction': 0.8116777316557674, 'bagging_fraction': 0.6791174812689909, 'bagging_freq': 7, 'lambda_l1': 7.143928763325178e-05, 'lambda_l2': 1.8355938120782364e-05}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 07:24:09,653] Trial 21 finished with value: 0.6847686985732014 and parameters: {'learning_rate': 0.09239040521600711, 'num_leaves': 78, 'feature_fraction': 0.6685949092949848, 'bagging_fraction': 0.8343842325328656, 'bagging_freq': 1, 'lambda_l1': 6.871051311008679e-08, 'lambda_l2': 7.149681653858625e-08}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 07:54:58,256] Trial 22 finished with value: 0.6859306097257409 and parameters: {'learning_rate': 0.08950393373538394, 'num_leaves': 69, 'feature_fraction': 0.654849051713509, 'bagging_fraction': 0.7714413925279848, 'bagging_freq': 2, 'lambda_l1': 7.2383522291632e-08, 'lambda_l2': 9.895561855869603e-08}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 08:23:38,521] Trial 23 finished with value: 0.687064386454432 and parameters: {'learning_rate': 0.07732599400775621, 'num_leaves': 56, 'feature_fraction': 0.7382111372505245, 'bagging_fraction': 0.8152666687898747, 'bagging_freq': 4, 'lambda_l1': 1.6386084797390463e-08, 'lambda_l2': 5.212155689826573e-06}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 09:00:11,598] Trial 24 finished with value: 0.6849479625908891 and parameters: {'learning_rate': 0.06652252739364334, 'num_leaves': 82, 'feature_fraction': 0.6795163336982701, 'bagging_fraction': 0.829850549060535, 'bagging_freq': 3, 'lambda_l1': 3.053328226066427e-07, 'lambda_l2': 0.0014187993714351946}. Best is trial 13 with value: 0.6845614699296525.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 09:37:08,605] Trial 25 finished with value: 0.6830237637072452 and parameters: {'learning_rate': 0.06436419745931313, 'num_leaves': 83, 'feature_fraction': 0.670787164561559, 'bagging_fraction': 0.8862372384842653, 'bagging_freq': 2, 'lambda_l1': 2.1147966897639476e-07, 'lambda_l2': 0.0006020821911774535}. Best is trial 25 with value: 0.6830237637072452.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 10:17:28,461] Trial 26 finished with value: 0.6844791078452156 and parameters: {'learning_rate': 0.05190180527695158, 'num_leaves': 93, 'feature_fraction': 0.6439180221012731, 'bagging_fraction': 0.9343615397090089, 'bagging_freq': 1, 'lambda_l1': 2.7021862363779376e-07, 'lambda_l2': 5.091957259302073e-05}. Best is trial 25 with value: 0.6830237637072452.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 10:59:59,709] Trial 27 finished with value: 0.685275954481684 and parameters: {'learning_rate': 0.04976743962733736, 'num_leaves': 100, 'feature_fraction': 0.6362829494239381, 'bagging_fraction': 0.9340451187618902, 'bagging_freq': 2, 'lambda_l1': 3.0030311665241474e-06, 'lambda_l2': 7.250562087527799e-05}. Best is trial 25 with value: 0.6830237637072452.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 11:39:57,321] Trial 28 finished with value: 0.6843378898721945 and parameters: {'learning_rate': 0.05423197880599518, 'num_leaves': 92, 'feature_fraction': 0.6056886932689238, 'bagging_fraction': 0.8846759782388062, 'bagging_freq': 1, 'lambda_l1': 2.7798608170613864e-07, 'lambda_l2': 0.000681192231712015}. Best is trial 25 with value: 0.6830237637072452.\n",
      "c:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "[I 2025-10-13 12:20:34,226] Trial 29 finished with value: 0.6863574785825858 and parameters: {'learning_rate': 0.04827941399599423, 'num_leaves': 93, 'feature_fraction': 0.6054350146882117, 'bagging_fraction': 0.9490485753316685, 'bagging_freq': 2, 'lambda_l1': 0.000184926987283051, 'lambda_l2': 0.0009096984276474835}. Best is trial 25 with value: 0.6830237637072452.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna study complete in 62535.20 seconds.\n",
      "Best RMSE: 0.6830237637072452\n",
      "Best Hyperparameters: {'learning_rate': 0.06436419745931313, 'num_leaves': 83, 'feature_fraction': 0.670787164561559, 'bagging_fraction': 0.8862372384842653, 'bagging_freq': 2, 'lambda_l1': 2.1147966897639476e-07, 'lambda_l2': 0.0006020821911774535}\n",
      "\n",
      " TRAINING ENSEMBLE OF MODELS...\n",
      "Training final LightGBM model...\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[7896]\tvalid_0's rmse: 0.677654\n",
      "LightGBM trained and saved in 8692.09 seconds.\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 107\u001b[0m\n\u001b[0;32m    105\u001b[0m xgb_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg:squarederror\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_metric\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrmse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m}\n\u001b[0;32m    106\u001b[0m xgb_model \u001b[38;5;241m=\u001b[39m xgb\u001b[38;5;241m.\u001b[39mXGBRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mxgb_params)\n\u001b[1;32m--> 107\u001b[0m \u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_part\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    108\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(xgb_model, BASE_PATH \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mxgb_final_model.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXGBoost trained and saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\xgboost\\core.py:729\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    727\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    728\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 729\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "BASE_PATH = r'C:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon'\n",
    "DATA_PATH = BASE_PATH + r'\\Dataset'\n",
    "TRAIN_PATH = DATA_PATH + r'\\train.csv'\n",
    "TEST_PATH = DATA_PATH + r'\\test.csv'\n",
    "TRAIN_IMG_FEATURES_PATH = BASE_PATH + r'\\train_image_features.npy'\n",
    "TEST_IMG_FEATURES_PATH = BASE_PATH + r'\\test_image_features.npy'\n",
    "print(\"Environment setup is complete.\")\n",
    "print(\"ENGINEERING ALL FEATURES\")\n",
    "start_time = time.time()\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "all_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "all_df['original_content'] = all_df['catalog_content'].fillna('')\n",
    "all_df['clean_content'] = all_df['original_content'].str.lower()\n",
    "all_df['text_length'] = all_df['original_content'].str.len()\n",
    "all_df['word_count'] = all_df['original_content'].apply(lambda x: len(x.split()))\n",
    "all_df['capital_ratio'] = all_df['original_content'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1e-6))\n",
    "def extract_ipq(text):\n",
    "    text = str(text).lower()\n",
    "    patterns = [r'pack of (\\d+)', r'(\\d+)\\s*pack', r'(\\d+)\\s*count', r'set of (\\d+)', r'(\\d+)\\s*ct', r'(\\d+)\\s*pk']\n",
    "    for p in patterns:\n",
    "        match = re.search(p, text)\n",
    "        if match: return int(match.group(1))\n",
    "    return 1\n",
    "all_df['ipq'] = all_df['clean_content'].apply(extract_ipq)\n",
    "keywords = {\n",
    "    'quality': ['premium', 'organic', 'heavy-duty', 'professional', 'gourmet', 'handmade', 'luxury'],\n",
    "    'bundling': ['set', 'bundle', 'kit', 'combo', 'pack'],\n",
    "    'condition': ['refurbished', 'new', 'generic', 'compatible']\n",
    "}\n",
    "for category, words in keywords.items():\n",
    "    all_df[f'kw_{category}'] = all_df['clean_content'].apply(lambda x: 1 if any(word in x for word in words) else 0)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=30000, stop_words='english', token_pattern=r'\\b[a-zA-Z0-9]+\\b')\n",
    "text_features_tfidf = tfidf_vectorizer.fit_transform(all_df['clean_content'])\n",
    "additional_features_df = all_df[['text_length', 'word_count', 'capital_ratio', 'ipq', 'kw_quality', 'kw_bundling', 'kw_condition']]\n",
    "additional_features_sparse = csr_matrix(additional_features_df.values)\n",
    "full_text_features = hstack([text_features_tfidf, additional_features_sparse], format='csr')\n",
    "\n",
    "train_image_features = np.load(TRAIN_IMG_FEATURES_PATH)\n",
    "test_image_features = np.load(TEST_IMG_FEATURES_PATH)\n",
    "train_image_sparse = csr_matrix(train_image_features)\n",
    "test_image_sparse = csr_matrix(test_image_features)\n",
    "\n",
    "x_train_final = hstack([full_text_features[:len(train_df)], train_image_sparse], format='csr')\n",
    "x_test_final = hstack([full_text_features[len(train_df):], test_image_sparse], format='csr')\n",
    "y_train = np.log1p(train_df['price'])\n",
    "\n",
    "print(f\"Feature engineering complete in {time.time() - start_time:.2f} seconds.\")\n",
    "print(f\"Final training data shape: {x_train_final.shape}\")\n",
    "print(\"\\n FINDING BEST HYPERPARAMETERS WITH OPTUNA\")\n",
    "start_time = time.time()\n",
    "x_train_part, x_val, y_train_part, y_val = train_test_split(x_train_final, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'rmse',\n",
    "        'n_estimators': 2000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 20, 100),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.6, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'lambda_l1': trial.suggest_float('lambda_l1', 1e-8, 10.0, log=True),\n",
    "        'lambda_l2': trial.suggest_float('lambda_l2', 1e-8, 10.0, log=True),\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'seed': 42\n",
    "    }\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(x_train_part, y_train_part, eval_set=[(x_val, y_val)], eval_metric='rmse', callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    preds = model.predict(x_val)\n",
    "    rmse = np.sqrt(np.mean((y_val - preds)**2))\n",
    "    return rmse\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30) \n",
    "\n",
    "best_lgbm_params = study.best_params\n",
    "print(f\"Optuna study complete in {time.time() - start_time:.2f} seconds.\")\n",
    "print(f\"Best RMSE: {study.best_value}\")\n",
    "print(f\"Best Hyperparameters: {best_lgbm_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d485590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup is complete.\n",
      "\n",
      " ENGINEERING ALL FEATURES\n",
      "Feature engineering complete in 123.90 seconds.\n",
      "\n",
      "PHASE 3: TRAINING FINAL ENSEMBLE OF MODELS...\n",
      "Training final LightGBM model\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[2886]\tvalid_0's rmse: 0.684679\n",
      "LightGBM trained and saved in 5151.03 seconds.\n",
      "Training HistGradientBoostingRegressor model\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 15.7 GiB for an array with shape (67500, 31287) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 76\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining HistGradientBoostingRegressor model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 76\u001b[0m x_train_part_dense \u001b[38;5;241m=\u001b[39m \u001b[43mx_train_part\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m hist_params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.05\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_iter\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2000\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_leaf_nodes\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m60\u001b[39m,\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation_fraction\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mearly_stopping\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_iter_no_change\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_state\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m42\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mverbose\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     81\u001b[0m }\n\u001b[0;32m     82\u001b[0m hist_model \u001b[38;5;241m=\u001b[39m HistGradientBoostingRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhist_params)\n",
      "File \u001b[1;32mc:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1167\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1166\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1167\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon\\venv\\lib\\site-packages\\scipy\\sparse\\_base.py:1354\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 15.7 GiB for an array with shape (67500, 31287) and data type float64"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "import lightgbm as lgb\n",
    "BASE_PATH = r'C:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon'\n",
    "DATA_PATH = BASE_PATH + r'\\Dataset'\n",
    "TRAIN_PATH = DATA_PATH + r'\\train.csv'\n",
    "TEST_PATH = DATA_PATH + r'\\test.csv'\n",
    "TRAIN_IMG_FEATURES_PATH = BASE_PATH + r'\\train_image_features.npy'\n",
    "TEST_IMG_FEATURES_PATH = BASE_PATH + r'\\test_image_features.npy'\n",
    "print(\"Environment setup is complete.\")\n",
    "print(\"\\n ENGINEERING ALL FEATURES\")\n",
    "start_time = time.time()\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "all_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "all_df['original_content'] = all_df['catalog_content'].fillna('')\n",
    "all_df['clean_content'] = all_df['original_content'].str.lower()\n",
    "all_df['text_length'] = all_df['original_content'].str.len()\n",
    "all_df['word_count'] = all_df['original_content'].apply(lambda x: len(x.split()))\n",
    "all_df['capital_ratio'] = all_df['original_content'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1e-6))\n",
    "def extract_ipq(text):\n",
    "    text = str(text).lower()\n",
    "    patterns = [r'pack of (\\d+)', r'(\\d+)\\s*pack', r'(\\d+)\\s*count', r'set of (\\d+)', r'(\\d+)\\s*ct', r'(\\d+)\\s*pk']\n",
    "    for p in patterns:\n",
    "        match = re.search(p, text)\n",
    "        if match: return int(match.group(1))\n",
    "    return 1\n",
    "all_df['ipq'] = all_df['clean_content'].apply(extract_ipq)\n",
    "keywords = {\n",
    "    'quality': ['premium', 'organic', 'heavy-duty', 'professional', 'gourmet', 'handmade', 'luxury'],\n",
    "    'bundling': ['set', 'bundle', 'kit', 'combo', 'pack'],\n",
    "    'condition': ['refurbished', 'new', 'generic', 'compatible']\n",
    "}\n",
    "for category, words in keywords.items():\n",
    "    all_df[f'kw_{category}'] = all_df['clean_content'].apply(lambda x: 1 if any(word in x for word in words) else 0)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=30000, stop_words='english', token_pattern=r'\\b[a-zA-Z0-9]+\\b')\n",
    "text_features_tfidf = tfidf_vectorizer.fit_transform(all_df['clean_content'])\n",
    "additional_features_df = all_df[['text_length', 'word_count', 'capital_ratio', 'ipq', 'kw_quality', 'kw_bundling', 'kw_condition']]\n",
    "additional_features_sparse = csr_matrix(additional_features_df.values)\n",
    "full_text_features = hstack([text_features_tfidf, additional_features_sparse], format='csr')\n",
    "\n",
    "train_image_features = np.load(TRAIN_IMG_FEATURES_PATH)\n",
    "test_image_features = np.load(TEST_IMG_FEATURES_PATH)\n",
    "train_image_sparse = csr_matrix(train_image_features)\n",
    "test_image_sparse = csr_matrix(test_image_features)\n",
    "\n",
    "x_train_final = hstack([full_text_features[:len(train_df)], train_image_sparse], format='csr')\n",
    "x_test_final = hstack([full_text_features[len(train_df):], test_image_sparse], format='csr')\n",
    "y_train = np.log1p(train_df['price'])\n",
    "\n",
    "print(f\"Feature engineering complete in {time.time() - start_time:.2f} seconds.\")\n",
    "print(\"\\n TRAINING FINAL LIGHTGBM MODEL\")\n",
    "x_train_part, x_val, y_train_part, y_val = train_test_split(x_train_final, y_train, test_size=0.1, random_state=42)\n",
    "best_lgbm_params = {\n",
    "    'learning_rate': 0.06436419745931313, 'num_leaves': 83,\n",
    "    'feature_fraction': 0.670787164561559, 'bagging_fraction': 0.8862372384842653,\n",
    "    'bagging_freq': 2, 'lambda_l1': 2.1147966897639476e-07, 'lambda_l2': 0.0006020821911774535\n",
    "}\n",
    "print(\"Training final LightGBM model\")\n",
    "start_time = time.time()\n",
    "lgbm_final_params = {**best_lgbm_params, 'n_estimators': 15000, 'objective': 'regression_l1', 'metric': 'rmse', 'seed': 42, 'n_jobs': -1, 'verbose': -1}\n",
    "lgbm_model = lgb.LGBMRegressor(**lgbm_final_params)\n",
    "lgbm_model.fit(x_train_part, y_train_part, eval_set=[(x_val, y_val)], eval_metric='rmse', callbacks=[lgb.early_stopping(100, verbose=True)])\n",
    "joblib.dump(lgbm_model, BASE_PATH + r'\\lgbm_final_model.pkl')\n",
    "print(f\"LightGBM trained and saved in {time.time() - start_time:.2f} seconds.\")\n",
    "\n",
    "print(\"\\n GENERATING FINAL SUBMISSION\")\n",
    "predictions_log = lgbm_model.predict(x_test_final)\n",
    "final_predictions = np.expm1(predictions_log)\n",
    "final_predictions[final_predictions < 0] = 0\n",
    "submission_df = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': final_predictions})\n",
    "SUBMISSION_PATH = BASE_PATH + r'\\final_lgbm_only_submission.csv'\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(f\"\\n Final submission file has been saved to: {SUBMISSION_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664a472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-creating feature set\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "BASE_PATH = r'C:\\Users\\inb20\\OneDrive\\Desktop\\Amazon Hackathon'\n",
    "DATA_PATH = BASE_PATH + r'\\Dataset'\n",
    "TRAIN_PATH = DATA_PATH + r'\\train.csv'\n",
    "TEST_PATH = DATA_PATH + r'\\test.csv'\n",
    "TRAIN_IMG_FEATURES_PATH = BASE_PATH + r'\\train_image_features.npy'\n",
    "TEST_IMG_FEATURES_PATH = BASE_PATH + r'\\test_image_features.npy'\n",
    "LGBM_MODEL_PATH = BASE_PATH + r'\\lgbm_final_model.pkl'\n",
    "print(\"Re-creating feature set\")\n",
    "start_time = time.time()\n",
    "train_df = pd.read_csv(TRAIN_PATH)\n",
    "test_df = pd.read_csv(TEST_PATH)\n",
    "all_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "all_df['original_content'] = all_df['catalog_content'].fillna('')\n",
    "all_df['clean_content'] = all_df['original_content'].str.lower()\n",
    "all_df['text_length'] = all_df['original_content'].str.len()\n",
    "all_df['word_count'] = all_df['original_content'].apply(lambda x: len(x.split()))\n",
    "all_df['capital_ratio'] = all_df['original_content'].apply(lambda x: sum(1 for c in x if c.isupper()) / (len(x) + 1e-6))\n",
    "def extract_ipq(text):\n",
    "    text = str(text).lower()\n",
    "    patterns = [r'pack of (\\d+)', r'(\\d+)\\s*pack', r'(\\d+)\\s*count', r'set of (\\d+)', r'(\\d+)\\s*ct', r'(\\d+)\\s*pk']\n",
    "    for p in patterns:\n",
    "        match = re.search(p, text)\n",
    "        if match: return int(match.group(1))\n",
    "    return 1\n",
    "all_df['ipq'] = all_df['clean_content'].apply(extract_ipq)\n",
    "keywords = {\n",
    "    'quality': ['premium', 'organic', 'heavy-duty', 'professional', 'gourmet', 'handmade', 'luxury'],\n",
    "    'bundling': ['set', 'bundle', 'kit', 'combo', 'pack'],\n",
    "    'condition': ['refurbished', 'new', 'generic', 'compatible']\n",
    "}\n",
    "for category, words in keywords.items():\n",
    "    all_df[f'kw_{category}'] = all_df['clean_content'].apply(lambda x: 1 if any(word in x for word in words) else 0)\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=30000, stop_words='english', token_pattern=r'\\b[a-zA-Z0-9]+\\b')\n",
    "text_features_tfidf = tfidf_vectorizer.fit_transform(all_df['clean_content'])\n",
    "additional_features_df = all_df[['text_length', 'word_count', 'capital_ratio', 'ipq', 'kw_quality', 'kw_bundling', 'kw_condition']]\n",
    "additional_features_sparse = csr_matrix(additional_features_df.values)\n",
    "full_text_features = hstack([text_features_tfidf, additional_features_sparse], format='csr')\n",
    "train_image_features = np.load(TRAIN_IMG_FEATURES_PATH)\n",
    "test_image_features = np.load(TEST_IMG_FEATURES_PATH)\n",
    "train_image_sparse = csr_matrix(train_image_features)\n",
    "test_image_sparse = csr_matrix(test_image_features)\n",
    "x_test_final = hstack([full_text_features[len(train_df):], test_image_sparse], format='csr')\n",
    "print(f\"Feature set re-created in {time.time() - start_time:.2f} seconds.\")\n",
    "print(\"Loading the pre-trained LightGBM model\")\n",
    "try:\n",
    "    lgbm_model = joblib.load(LGBM_MODEL_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(\"CRITICAL ERROR: 'lgbm_final_model.pkl' not found. Cannot create submission.\")\n",
    "    exit()\n",
    "print(\"Making final predictions\")\n",
    "predictions_log = lgbm_model.predict(x_test_final)\n",
    "final_predictions = np.expm1(predictions_log)\n",
    "final_predictions[final_predictions < 0] = 0\n",
    "submission_df = pd.DataFrame({'sample_id': test_df['sample_id'], 'price': final_predictions})\n",
    "SUBMISSION_PATH = BASE_PATH + r'\\Final_submission.csv'\n",
    "submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(f\"Submission file has been saved to: {SUBMISSION_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
